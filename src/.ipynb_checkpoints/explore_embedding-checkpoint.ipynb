{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we are going to show you the embeddings before and after training,\n",
    "## including the initial embed(before graph embed), graph embed(before MLP) and MLP embed(before output)\n",
    "\n",
    "### You can also run tSNE_citation.py under the same folder \n",
    "### If you want to test on graph datasets, you can also run tSNE_tudataset.py \n",
    "### Ideally you do not need to add command line parameters since the program will generate results on given test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import statistics\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from optimal_R import option, all_possible_concatenation\n",
    "from graph_property import G_property, binning\n",
    "from model.GNN import Net, debug_MLP\n",
    "from utils import max_len_arr\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model(data)[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "      0    1     2     3    4\n",
      "0  SAGE  GIN   GIN   GCN  GIN\n",
      "1   GIN  MLP  SAGE  SAGE  GIN\n",
      "2   GIN  GIN  SAGE   GIN  GCN\n",
      "3   GCN  GIN   GIN   GIN  GIN\n",
      "4   GIN  GIN   GIN   GIN  MLP\n",
      "GIN\n",
      "Epoch: 001, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 002, Train: 0.4167, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 003, Train: 0.3917, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 004, Train: 0.5417, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 005, Train: 0.6333, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 006, Train: 0.6333, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 007, Train: 0.6417, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 008, Train: 0.6583, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 009, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 010, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 011, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 012, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 013, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 014, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 015, Train: 0.6667, Val: 0.6860, Test: 0.6580\n",
      "Epoch: 016, Train: 0.6667, Val: 0.6900, Test: 0.6630\n",
      "Epoch: 017, Train: 0.6750, Val: 0.6980, Test: 0.6730\n",
      "Epoch: 018, Train: 0.6833, Val: 0.6980, Test: 0.6730\n",
      "Epoch: 019, Train: 0.6917, Val: 0.6980, Test: 0.6730\n",
      "Epoch: 020, Train: 0.6917, Val: 0.7020, Test: 0.6880\n",
      "Epoch: 021, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 022, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 023, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 024, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 025, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 026, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 027, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 028, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 029, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 030, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 031, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 032, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 033, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 034, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 035, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 036, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 037, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 038, Train: 0.7000, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 039, Train: 0.7083, Val: 0.7040, Test: 0.6860\n",
      "Epoch: 040, Train: 0.7167, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 041, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 042, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 043, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 044, Train: 0.7000, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 045, Train: 0.7000, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 046, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 047, Train: 0.6833, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 048, Train: 0.6750, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 049, Train: 0.6750, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 050, Train: 0.6750, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 051, Train: 0.6833, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 052, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 053, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 054, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 055, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 056, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 057, Train: 0.7000, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 058, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 059, Train: 0.6917, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 060, Train: 0.6750, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 061, Train: 0.7167, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 062, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 063, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 064, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 065, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 066, Train: 0.7250, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 067, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 068, Train: 0.7000, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 069, Train: 0.7083, Val: 0.7100, Test: 0.6940\n",
      "Epoch: 070, Train: 0.7167, Val: 0.7120, Test: 0.6790\n",
      "Epoch: 071, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 072, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 073, Train: 0.7000, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 074, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 075, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 076, Train: 0.7000, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 077, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 078, Train: 0.7083, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 079, Train: 0.7333, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 080, Train: 0.7333, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 081, Train: 0.7333, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 082, Train: 0.7333, Val: 0.7140, Test: 0.6860\n",
      "Epoch: 083, Train: 0.7583, Val: 0.7160, Test: 0.7110\n",
      "Epoch: 084, Train: 0.7417, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 085, Train: 0.7250, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 086, Train: 0.7583, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 087, Train: 0.7583, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 088, Train: 0.7417, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 089, Train: 0.7500, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 090, Train: 0.7417, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 091, Train: 0.7250, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 092, Train: 0.7333, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 093, Train: 0.7250, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 094, Train: 0.7333, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 095, Train: 0.7333, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 096, Train: 0.7000, Val: 0.7240, Test: 0.7070\n",
      "Epoch: 097, Train: 0.7583, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 098, Train: 0.7250, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 099, Train: 0.7167, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 100, Train: 0.7250, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 101, Train: 0.6167, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 102, Train: 0.5833, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 103, Train: 0.5833, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 104, Train: 0.6167, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 105, Train: 0.6333, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 106, Train: 0.6750, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 107, Train: 0.7333, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 108, Train: 0.7500, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 109, Train: 0.7417, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 110, Train: 0.7083, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 111, Train: 0.7000, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 112, Train: 0.7167, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 113, Train: 0.7417, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 114, Train: 0.7417, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 115, Train: 0.7500, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 116, Train: 0.7667, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 117, Train: 0.7583, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 118, Train: 0.7417, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 119, Train: 0.7583, Val: 0.7260, Test: 0.7120\n",
      "Epoch: 120, Train: 0.7583, Val: 0.7380, Test: 0.7260\n",
      "Epoch: 121, Train: 0.7667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 122, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 123, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 124, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 125, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 126, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 127, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 128, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 129, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 130, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 131, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 132, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 133, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 134, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 135, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 136, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 137, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 138, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 139, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 140, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 141, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 142, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 143, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 144, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 145, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 146, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 147, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 148, Train: 0.7000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 149, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 150, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 151, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 152, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 153, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 154, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 155, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 156, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 157, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 158, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 159, Train: 0.7500, Val: 0.7420, Test: 0.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 161, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 162, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 163, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 164, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 165, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 166, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 167, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 168, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 169, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 170, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 171, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 172, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 173, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 174, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 175, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 176, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 177, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 178, Train: 0.7000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 179, Train: 0.7000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 180, Train: 0.7000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 181, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 182, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 183, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 184, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 185, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 186, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 187, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 188, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 189, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 190, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 191, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 192, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 193, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 194, Train: 0.7667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 195, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 196, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 197, Train: 0.7667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 198, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 199, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 200, Train: 0.7667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 201, Train: 0.7333, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 202, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 203, Train: 0.7083, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 204, Train: 0.7000, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 205, Train: 0.6833, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 206, Train: 0.6417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 207, Train: 0.6500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 208, Train: 0.6250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 209, Train: 0.6750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 210, Train: 0.6667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 211, Train: 0.6667, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 212, Train: 0.7167, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 213, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 214, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 215, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 216, Train: 0.7750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 217, Train: 0.7750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 218, Train: 0.7750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 219, Train: 0.7750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 220, Train: 0.7750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 221, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 222, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 223, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 224, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 225, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 226, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 227, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 228, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 229, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 230, Train: 0.6750, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 231, Train: 0.6917, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 232, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 233, Train: 0.7250, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 234, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 235, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 236, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 237, Train: 0.7500, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 238, Train: 0.7417, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 239, Train: 0.7583, Val: 0.7420, Test: 0.7230\n",
      "Epoch: 240, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 241, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 242, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 243, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 244, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 245, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 246, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 247, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 248, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 249, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 250, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 251, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 252, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 253, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 254, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 255, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 256, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 257, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 258, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 259, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 260, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 261, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 262, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 263, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 264, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 265, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 266, Train: 0.7917, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 267, Train: 0.7833, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 268, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 269, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 270, Train: 0.7833, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 271, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 272, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 273, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 274, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 275, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 276, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 277, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 278, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 279, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 280, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 281, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 282, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 283, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 284, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 285, Train: 0.6000, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 286, Train: 0.4917, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 287, Train: 0.5083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 288, Train: 0.4333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 289, Train: 0.4417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 290, Train: 0.4500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 291, Train: 0.4917, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 292, Train: 0.6667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 293, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 294, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 295, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 296, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 297, Train: 0.6500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 298, Train: 0.6583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 299, Train: 0.6667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 300, Train: 0.6583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 301, Train: 0.6667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 302, Train: 0.6500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 303, Train: 0.6583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 304, Train: 0.6667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 305, Train: 0.6917, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 306, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 307, Train: 0.7167, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 308, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 309, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 310, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 311, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 312, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 313, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 314, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 315, Train: 0.1417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 316, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 317, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 318, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 319, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 320, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 321, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 322, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 323, Train: 0.7167, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 324, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 325, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 326, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 327, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 328, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 329, Train: 0.6750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 330, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 331, Train: 0.7333, Val: 0.7440, Test: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 332, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 333, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 334, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 335, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 336, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 337, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 338, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 339, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 340, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 341, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 342, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 343, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 344, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 345, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 346, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 347, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 348, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 349, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 350, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 351, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 352, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 353, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 354, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 355, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 356, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 357, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 358, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 359, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 360, Train: 0.7083, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 361, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 362, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 363, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 364, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 365, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 366, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 367, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 368, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 369, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 370, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 371, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 372, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 373, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 374, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 375, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 376, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 377, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 378, Train: 0.7333, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 379, Train: 0.7167, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 380, Train: 0.7250, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 381, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 382, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 383, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 384, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 385, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 386, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 387, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 388, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 389, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 390, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 391, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 392, Train: 0.7667, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 393, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 394, Train: 0.7750, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 395, Train: 0.7417, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 396, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 397, Train: 0.7583, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 398, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 399, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 400, Train: 0.7833, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 401, Train: 0.7500, Val: 0.7440, Test: 0.6930\n",
      "Epoch: 402, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 403, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 404, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 405, Train: 0.7167, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 406, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 407, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 408, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 409, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 410, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 411, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 412, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 413, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 414, Train: 0.7167, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 415, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 416, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 417, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 418, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 419, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 420, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 421, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 422, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 423, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 424, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 425, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 426, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 427, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 428, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 429, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 430, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 431, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 432, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 433, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 434, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 435, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 436, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 437, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 438, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 439, Train: 0.7000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 440, Train: 0.7000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 441, Train: 0.7000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 442, Train: 0.7083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 443, Train: 0.7000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 444, Train: 0.7083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 445, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 446, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 447, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 448, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 449, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 450, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 451, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 452, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 453, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 454, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 455, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 456, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 457, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 458, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 459, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 460, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 461, Train: 0.7167, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 462, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 463, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 464, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 465, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 466, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 467, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 468, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 469, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 470, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 471, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 472, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 473, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 474, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 475, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 476, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 477, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 478, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 479, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 480, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 481, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 482, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 483, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 484, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 485, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 486, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 487, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 488, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 489, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 490, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 491, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 492, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 493, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 494, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 495, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 496, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 497, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 498, Train: 0.7583, Val: 0.7460, Test: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 500, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 501, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 502, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 503, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 504, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 505, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 506, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 507, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 508, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 509, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 510, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 511, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 512, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 513, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 514, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 515, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 516, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 517, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 518, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 519, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 520, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 521, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 522, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 523, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 524, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 525, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 526, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 527, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 528, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 529, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 530, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 531, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 532, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 533, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 534, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 535, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 536, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 537, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 538, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 539, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 540, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 541, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 542, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 543, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 544, Train: 0.7000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 545, Train: 0.6833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 546, Train: 0.6917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 547, Train: 0.6750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 548, Train: 0.6667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 549, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 550, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 551, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 552, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 553, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 554, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 555, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 556, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 557, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 558, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 559, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 560, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 561, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 562, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 563, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 564, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 565, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 566, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 567, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 568, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 569, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 570, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 571, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 572, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 573, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 574, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 575, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 576, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 577, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 578, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 579, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 580, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 581, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 582, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 583, Train: 0.6917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 584, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 585, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 586, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 587, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 588, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 589, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 590, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 591, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 592, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 593, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 594, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 595, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 596, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 597, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 598, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 599, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 600, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 601, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 602, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 603, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 604, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 605, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 606, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 607, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 608, Train: 0.7250, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 609, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 610, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 611, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 612, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 613, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 614, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 615, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 616, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 617, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 618, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 619, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 620, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 621, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 622, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 623, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 624, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 625, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 626, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 627, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 628, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 629, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 630, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 631, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 632, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 633, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 634, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 635, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 636, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 637, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 638, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 639, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 640, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 641, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 642, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 643, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 644, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 645, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 646, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 647, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 648, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 649, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 650, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 651, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 652, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 653, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 654, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 655, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 656, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 657, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 658, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 659, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 660, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 661, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 662, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 663, Train: 0.7750, Val: 0.7460, Test: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 664, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 665, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 666, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 667, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 668, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 669, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 670, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 671, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 672, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 673, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 674, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 675, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 676, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 677, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 678, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 679, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 680, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 681, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 682, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 683, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 684, Train: 0.7167, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 685, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 686, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 687, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 688, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 689, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 690, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 691, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 692, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 693, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 694, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 695, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 696, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 697, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 698, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 699, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 700, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 701, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 702, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 703, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 704, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 705, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 706, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 707, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 708, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 709, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 710, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 711, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 712, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 713, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 714, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 715, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 716, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 717, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 718, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 719, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 720, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 721, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 722, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 723, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 724, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 725, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 726, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 727, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 728, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 729, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 730, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 731, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 732, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 733, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 734, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 735, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 736, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 737, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 738, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 739, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 740, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 741, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 742, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 743, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 744, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 745, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 746, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 747, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 748, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 749, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 750, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 751, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 752, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 753, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 754, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 755, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 756, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 757, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 758, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 759, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 760, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 761, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 762, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 763, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 764, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 765, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 766, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 767, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 768, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 769, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 770, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 771, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 772, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 773, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 774, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 775, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 776, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 777, Train: 0.8083, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 778, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 779, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 780, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 781, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 782, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 783, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 784, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 785, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 786, Train: 0.7917, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 787, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 788, Train: 0.7750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 789, Train: 0.7667, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 790, Train: 0.7833, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 791, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 792, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 793, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 794, Train: 0.7583, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 795, Train: 0.7333, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 796, Train: 0.6750, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 797, Train: 0.7167, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 798, Train: 0.7417, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 799, Train: 0.7500, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 800, Train: 0.8000, Val: 0.7460, Test: 0.7120\n",
      "Epoch: 801, Train: 0.7500, Val: 0.7460, Test: 0.7120\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5cf2293ee881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# draw tSNE pictures here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m#y = np.array(property_j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mX_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "path = osp.join('/home/jiaqing//Fea2Fea/data/')\n",
    "test_case = [(2, 1),(1, 3)]\n",
    "\n",
    "dataset_name = ['Cora', 'PubMed', 'Citeseer']\n",
    "for dataset in dataset_name:\n",
    "    d_name = dataset\n",
    "    dataset = Planetoid(path, name = dataset, transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    path = r'/home/jiaqing//Fea2Fea/Result/Planetoid/'\n",
    "    name = path + d_name + '_property.txt'\n",
    "    property_file = pd.read_csv(name, sep = '\\t')\n",
    "    for (j, i) in test_case:\n",
    "        print(i,j)\n",
    "        # find optimal graph embedding method according to each\n",
    "        # input graph feature and output graph feature\n",
    "        tmp_txt = pd.read_csv(path + d_name + '_optimal_method.txt', sep = '\\t', header = None) # array\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        embedding = 0\n",
    "        best_val_acc = test_acc = 0\n",
    "        t = 0\n",
    "        train_accu_plot = []\n",
    "        epoch_plot = []\n",
    "        print(tmp_txt)\n",
    "        print(tmp_txt[1][2])\n",
    "        # take the optimal embedding method as graph embedding\n",
    "        model = Net(embedding=tmp_txt[i][j]).to(device) if tmp_txt[i][j] != 'MLP' else debug_MLP().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)\n",
    "\n",
    "        property_i = np.array(property_file.iloc[:,[i]])\n",
    "        data.x = torch.tensor(property_i).float()\n",
    "\n",
    "        property_j = np.array(property_file.iloc[:,[j]])\n",
    "        tmp = binning(property_j, k = 6, data_len = len(data.y))\n",
    "        data.y = binning(property_j, k = 6, data_len = len(data.y))\n",
    "        data =  data.to(device)\n",
    "        for epoch in range(1, 3000):   \n",
    "            train()\n",
    "            train_acc, val_acc, tmp_test_acc = test()\n",
    "            #train_accu_plot.append(train_acc)\n",
    "            #epoch_plot.append(epoch)\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                test_acc = tmp_test_acc\n",
    "                embedding = model.latent\n",
    "                t = 0\n",
    "            t = t + 1\n",
    "            if t > 400:\n",
    "                break   \n",
    "            log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "            print(log.format(epoch, train_acc, best_val_acc, test_acc))\n",
    "\n",
    "        nb_classes = 6\n",
    "        confusion_matrix = torch.zeros(nb_classes,nb_classes)\n",
    "        pre_comb = torch.tensor([])\n",
    "        real_comb = torch.tensor([])\n",
    "\n",
    "        '''\n",
    "        #----- print macro-f1 score\n",
    "        with torch.no_grad():\n",
    "            logits, accs = model(), []\n",
    "            for _, mask in data('test_mask'):\n",
    "                pred = logits[mask].max(1)[1]\n",
    "                pre_comb = torch.cat((pre_comb, pred), 0)\n",
    "                real_comb = torch.cat((real_comb, data.y[mask]), 0)\n",
    "\n",
    "                #print(pred)\n",
    "                #print(data.y[mask])\n",
    "                for i in range(len(pred)):\n",
    "                    confusion_matrix[pred[i]][data.y[mask][i]] = confusion_matrix[pred[i]][data.y[mask][i]]+1\n",
    "            print(confusion_matrix)#\n",
    "            print(f1_score(pre_comb.numpy(), real_comb.numpy(), average='macro'))\n",
    "        '''\n",
    "\n",
    "        # draw tSNE pictures here:\n",
    "        x = embedding.detach().numpy()\n",
    "        #y = np.array(property_j)\n",
    "        X_tsne = TSNE(n_components=2,random_state=33).fit_transform(x)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.subplot(1,1,1,)\n",
    "\n",
    "        values = range(6)\n",
    "        cNorm  = colors.Normalize(vmin=0, vmax=values[-1])\n",
    "        scaMap = plt.cm.ScalarMappable(norm = cNorm  ,cmap = \"coolwarm\")\n",
    "\n",
    "        for k in range(6):  \n",
    "            colorval = scaMap.to_rgba(values[k])\n",
    "            ax.scatter(X_tsne[np.where(tmp.numpy() == k), 0], X_tsne[np.where(tmp.numpy() == k), 1] ,label = k, s =3, color = colorval)\n",
    "\n",
    "\n",
    "        handles,labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels, loc='upper right',fontsize = 7)\n",
    "        plt.xlabel(\"tSNE 1\",fontsize = 12)\n",
    "        plt.ylabel(\"tSNE 2\", fontsize = 12)\n",
    "        plt.tick_params(labelsize=12)\n",
    "        name2 = r'/home/jiaqing//FASG_KDD/Result/tSNE/'\n",
    "        plt.savefig(name2 + str(d_name)+\"_\"+ str(i)+ \"to\" + str(j) +\"_tSNE.eps\", dpi = 800, format = 'eps')\n",
    "        #plt.show()\n",
    "        #plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=tmp.numpy(), cmap = \"rainbow\")\n",
    "        #plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
